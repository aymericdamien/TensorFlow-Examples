{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Example\n",
    "\n",
    "Build a 2-hidden layers fully connected neural network (a.k.a multilayer perceptron) with TensorFlow v2.\n",
    "\n",
    "This example is using a low-level approach to better understand all mechanics behind building neural networks and the training process.\n",
    "\n",
    "- Author: Miguel Tomás\n",
    "- Project: https://github.com/aeonSolutions/TensorFlow-Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Overview\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" alt=\"nn\" style=\"width: 400px;\"/>\n",
    "\n",
    "This example is using a file csv dataset.  \n",
    "\n",
    "In this example, each dataset will be converted to float32, normalized to [0, 1] and flattened to a 1-D array of \"num_features\" features \n",
    "\n",
    "More info: https://github.com/aeonSolutions/TensorFlow-Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualize predictions.\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters initialization.\n",
    "num_classes = 100 # total classes : number of output results\n",
    "num_features = 0 # data features : number of samples(rows) for the variable set. a value of 0 loads from the dataset bellow\n",
    "\n",
    "# Training parameters.\n",
    "learning_rate = 0.1\n",
    "training_steps = 10000\n",
    "batch_size = 100\n",
    "display_step = 50\n",
    "\n",
    "#normalization of data\n",
    "normalizeDataValues=True\n",
    "normalizationType= \"mean\" # accepts: max, mean\n",
    "normalizationTypeBinary=True\n",
    "\n",
    "# Network parameters.\n",
    "n_hidden_1 = 512 # 1st layer number of neurons.\n",
    "n_hidden_2 = 512 # 2nd layer number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-09430af9c07a>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-09430af9c07a>\"\u001b[1;36m, line \u001b[1;32m38\u001b[0m\n\u001b[1;33m    else\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# predictions Data.\n",
    "df_predict_ds=pd.read_csv('./week3_exam_dataset_test.csv')\n",
    "\n",
    "data_predict_x = np.float32(df_predict_ds.values)\n",
    "\n",
    "# Training Data.\n",
    "df_tr=pd.read_csv('./week3_exam_dataset_train.csv')\n",
    "\n",
    "df_tr_raw_y= df_tr['y']\n",
    "if num_classes==0:\n",
    "    num_classes= df_tr_raw_y.shape[0]\n",
    "if num_features==0:\n",
    "    num_features= df_tr_raw_y.shape[0]\n",
    "else:\n",
    "    #TODO: fill possible empty values on the datasets\n",
    "    \n",
    "df_tr_raw_values_y = df_tr_raw_y.values\n",
    "data_tr_y = np.float32(df_tr_raw_values_y)\n",
    "\n",
    "df_tr_raw_x= df_tr.drop('y',1)\n",
    "df_tr_raw_values_x = df_tr_raw_x.values\n",
    "data_tr_x = np.float32(df_tr_raw_values_x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_tr_raw_x, df_tr_raw_y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Convert to float32.\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# Convert to float32.\n",
    "y_train, y_test = np.array(y_train, np.float32), np.array(y_test, np.float32)\n",
    "\n",
    "# Normalize data values to [0, 1] interval.\n",
    "if normalizeDataValues:\n",
    "    if normalizationType==\"max\":\n",
    "        maxVal_train=np.amax(x_train, axis=0)\n",
    "        maxVal_test=np.amax(x_test, axis=0)\n",
    "        maxVal_pred=np.amax(data_predict_x, axis=0)\n",
    "        maxVal=max(np.amax(maxVal_train, axis=0),np.amax(maxVal_test, axis=0),np.amax(maxVal_pred, axis=0))\n",
    "        if normalizationTypeBinary:\n",
    "            x_train, x_test,data_predict_x = np.where((x_train / maxVal_train)>=0.5,1,0), np.where((x_test / maxVal_test)>=0.5,1,0), np.where((data_predict_x / maxVal_pred)>=0.5,1,0)\n",
    "        else\n",
    "            x_train, x_test, data_predict_x = x_train / maxVal_train, x_test / maxVal_test, data_predict_x / maxVal_pred\n",
    "    else:\n",
    "        meanVal_test=np.mean(x_test, axis=0)\n",
    "        meanVal_train=np.mean(x_train, axis=0)\n",
    "        meanVal_pred=np.mean(data_predict_x, axis=0)\n",
    "        meanVal=max(np.amax(meanVal_train, axis=0),np.amax(meanVal_test, axis=0),np.amax(meanVal_pred, axis=0))\n",
    "        \n",
    "        if normalizationTypeBinary:\n",
    "            x_train, x_test,data_predict_x = np.where((x_train / meanVal_train)>=0.5,1,0), np.where((x_test / meanVal_test)>=0.5,1,0), np.where((data_predict_x / meanVal_pred)>=0.5,1,0)\n",
    "        else\n",
    "            x_train, x_test, data_predict_x = x_train / meanVal_train, x_test / meanVal_test, data_predict_x / meanVal_pred\n",
    "\n",
    "# Use tf.data API to shuffle and batch data.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF Model.\n",
    "class NeuralNet(Model):\n",
    "    # Set layers.\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # First fully-connected hidden layer.\n",
    "        self.fc1 = layers.Dense(n_hidden_1, activation=tf.nn.relu)\n",
    "        # First fully-connected hidden layer.\n",
    "        self.fc2 = layers.Dense(n_hidden_2, activation=tf.nn.relu)\n",
    "        # Second fully-connecter hidden layer.\n",
    "        self.out = layers.Dense(num_classes)\n",
    "\n",
    "    # Set forward pass.\n",
    "    def call(self, x, is_training=False):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        if not is_training:\n",
    "            # tf cross entropy expect logits without softmax, so only\n",
    "            # apply softmax when not training.\n",
    "            x = tf.nn.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Build neural network model.\n",
    "neural_net = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss.\n",
    "# Note that this will apply 'softmax' to the logits.\n",
    "def cross_entropy_loss(x, y):\n",
    "    # Convert labels to int 64 for tf cross-entropy function.\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    # Apply softmax to logits and compute cross-entropy.\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n",
    "    # Average loss across the batch.\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracyAvg(y_pred):\n",
    "    print(y_pred.numpy().shape)\n",
    "    \n",
    "    #convert to 1D array\n",
    "    y_pred_1d_array= y_pred.ravel()\n",
    "    \n",
    "    accCalc= np.full(y_pred_1d_array.shape, 0)\n",
    "    delta=np.amax(real_y_1d_array)-np.amin(real_y_1d_array)\n",
    "    \n",
    "    for i in range(len(y_pred_1d_array)):\n",
    "        accCalc[i]= abs(delta-y_pred_1d_array[i] - real_y_1d_array[i])\n",
    "    \n",
    "    return accCalc\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization process. \n",
    "def run_optimization(x, y):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        # Forward pass.\n",
    "        pred = neural_net(x, is_training=True)\n",
    "        # Compute loss.\n",
    "        loss = cross_entropy_loss(pred, y)\n",
    "        \n",
    "    # Variables to update, i.e. trainable variables.\n",
    "    trainable_variables = neural_net.trainable_variables\n",
    "\n",
    "    # Compute gradients.\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "def live_plot(steps, accuracy, figsize=(7,5), title=''):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.xlim(0, training_steps)\n",
    "    plt.ylim(0, 100)\n",
    "    steps= [float(i) for i in steps]\n",
    "    accuracy= [float(i) for i in accuracy]\n",
    "    \n",
    "    m=0\n",
    "    if len(steps) > 1:\n",
    "        plt.scatter(steps,accuracy, label='accuracy', color='k') \n",
    "        m, b = np.polyfit(steps, accuracy, 1)\n",
    "        plt.plot(steps, [x * m for x in steps] + b)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy %')\n",
    "    #plt.legend(loc='center left') # the plot evolves to the right\n",
    "    plt.show();\n",
    "    return m\n",
    "\n",
    "def ETC(start, steps):\n",
    "    time_elapsed = datetime.now() - start\n",
    "    eta= (training_steps-steps) / display_step * time_elapsed\n",
    "    #avgString = str(avg).split(\".\")[0]\n",
    "    \n",
    "    hours= int(eta.seconds/3600)\n",
    "    minutes= int((eta.seconds/60)-hours*60)\n",
    "    seconds = int(eta.seconds - minutes*60 -hours*3600)\n",
    "    return \"%sh, %s min and %s sec\" % (hours, minutes, seconds)\n",
    "\n",
    "def elapsedTime(elapsed):\n",
    "    hours= int(elapsed.seconds/3600)\n",
    "    minutes= int((elapsed.seconds/60)-hours*60)\n",
    "    seconds = int(elapsed.seconds - minutes*60 -hours*3600)\n",
    "    return \"%sh, %s min and %s sec\" % (hours, minutes, seconds)\n",
    "\n",
    "def progress(percent=0, width=30):\n",
    "    left = width * percent // 100\n",
    "    right = width - left\n",
    "    print('\\r[', '#' * left, ' ' * right, ']',\n",
    "          f' {percent:.0f}%\\n',\n",
    "          sep='', end='', flush=True)\n",
    "    \n",
    "def measureSkewness(series):    \n",
    "    result=\"\"\n",
    "    \n",
    "    if (series.skew() > 1 or series.skew() < -1):\n",
    "        result=\"Highly skewed distribution\"\n",
    "    elif((0.5 < series.skew() < 1) or (-1 < series.skew() < -0.5)):\n",
    "        result=\"Moderately skewed distribution\"\n",
    "    elif(-0.5 < series.skew() < 0.5):\n",
    "        result=\"Approximately symmetric distribution\"\n",
    "    result = result +\" ( \" +str(round(series.skew(),2))+\" )\"\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "[#                             ] 6%\n",
      "\n",
      "================= Iteration Stats ================\n",
      "                   step: 650 of 10000\n",
      "                   loss: 0.340414\n",
      "               accuracy: 82.00 %\n",
      "                    AVG: 80.38 %\n",
      "            Trend slope: 0.000\n",
      "         MAD Dispersion: nan\n",
      "               Skewness: Moderately skewed distribution ( -0.82 )\n",
      "\n",
      "================= Time           ================\n",
      "                Elapsed: 0h, 0 min and 9 sec\n",
      "                    ETC: 0h, 2 min and 15 sec\n",
      "\n",
      "================= Network Setup  ================\n",
      "      number of classes: 2\n",
      "     number of features: 14\n",
      "          learning rate: 0.1\n",
      "         training steps: 10000\n",
      "             batch size: 100\n",
      "1st layer n. of neurons: 512\n",
      "2st layer n. of neurons: 512\n",
      "        Normalized data: True, type: mean, Discrete Binary 0/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-5f941bd3b0d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Run the optimization to update W and b values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mrun_optimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-595e35d3d378>\u001b[0m in \u001b[0;36mrun_optimization\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Compute gradients.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Update W and b following gradients.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1065\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[0;32m   1066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1067\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1068\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MeanGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    264\u001b[0m     factor = _safe_shape_div(\n\u001b[0;32m    265\u001b[0m         math_ops.reduce_prod(input_shape), math_ops.reduce_prod(output_shape))\n\u001b[1;32m--> 266\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruediv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mtruediv\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1295\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m   \"\"\"\n\u001b[1;32m-> 1297\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_truediv_python3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_truediv_python3\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1234\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m       \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal_div\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mreal_div\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   7436\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7437\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7438\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   7439\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"RealDiv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7440\u001b[0m         tld.op_callbacks, x, y)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run training for the given number of steps.\n",
    "avgCounter=0\n",
    "avg=0.0\n",
    "\n",
    "steps=[]\n",
    "accuracyValue=[]\n",
    "\n",
    "start_time = datetime.now()\n",
    "totalStartTime=start_time\n",
    "    \n",
    "live_plot([0], [0])\n",
    "print(\"analysis started. Waiting for preliminary data. One moment please...\")\n",
    "progress(0) \n",
    "\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization(batch_x, batch_y)\n",
    "        \n",
    "    if step % display_step == 0:\n",
    "        pred = neural_net(batch_x, is_training=True)\n",
    "        loss = cross_entropy_loss(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        avgCounter+=1\n",
    "        avg+=acc*100\n",
    "        \n",
    "        steps.append(step)\n",
    "        accuracyValue.append(acc*100)\n",
    "        trendSlope= live_plot(steps, accuracyValue)\n",
    "        totalTime =datetime.now()- totalStartTime\n",
    "        \n",
    "        stats=pd.Series(accuracyValue)\n",
    "        \n",
    "        if int(step/training_steps*100)<100:\n",
    "            print(\"Running...\")\n",
    "        progress(int(step/training_steps*100)) \n",
    "        print(\"\")\n",
    "        print(\"================= Iteration Stats ================\")\n",
    "        print(\"                   step: %i of %i\" % (step,training_steps))\n",
    "        print(\"                   loss: %f\" % loss)\n",
    "        print(\"               accuracy: %.2f %%\" %  (acc*100))\n",
    "        print(\"                    AVG: %.2f %%\" % (avg/avgCounter))\n",
    "        print(\"            Trend slope: %.3f\" % (trendSlope))\n",
    "        print(\"         MAD Dispersion: \" + str(stats.mad()))\n",
    "        print(\"               Skewness: \" + measureSkewness(stats))\n",
    "        print(\"\")\n",
    "        print(\"================= Time           ================\")\n",
    "        print(\"                Elapsed: \" + elapsedTime(totalTime))\n",
    "        print(\"                    ETC: \" + ETC(start_time,step))\n",
    "        print(\"\")\n",
    "        print(\"================= Network Setup  ================\")\n",
    "        print(\"      number of classes: \"+ str(num_classes))\n",
    "        print(\"     number of features: \"+ str(num_features)) \n",
    "\n",
    "        print(\"          learning rate: \"+ str(learning_rate))\n",
    "        print(\"         training steps: \"+ str(training_steps))\n",
    "        print(\"             batch size: \"+ str(batch_size))\n",
    "\n",
    "        print(\"1st layer n. of neurons: \"+ str(n_hidden_1 ))\n",
    "        print(\"2st layer n. of neurons: \"+ str(n_hidden_2))\n",
    "        print(\"        Normalized data: \" + (\"True\" if normalizeDataValues else \"False\") +\", type: \" + normalizationType +\", \" +(\"Discrete Binary 0/1\" if normalizationType else \"Continuous range [0,1]\"))\n",
    "\n",
    "        start_time = datetime.now()\n",
    "print(\"\")\n",
    "print(\"Analysis finished.\")        \n",
    "#print(\"Final Average accuracy is %.2f %%\" % (avg/avgCounter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#!!! code bellow this line is not yet finished !!!\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Test model on validation set.\n",
    "print(x_test.shape)\n",
    "\n",
    "pred = neural_net(x_test, is_training=False)\n",
    "\n",
    "print(\"Accuracy of highest score in prediction vector\")\n",
    "print(\"         Test Accuracy: %.2f %%\" % (tf.math.round(100*accuracy(pred, y_test))))\n",
    "print(\"\")\n",
    "\n",
    "prediction= np.round(pred.numpy(),2)\n",
    "print(\"Model prediction shape:\"  + str(prediction.shape))\n",
    "print(\"   Model initial shape:\"  + str(y_test.shape))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Model prediction value:\")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = neural_net(data_predict_x)\n",
    "print(\"Model prediction: %i\" % np.argmax(predictions.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Author: Miguel Tomás \n",
    "\n",
    " License: Creative Commons \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
